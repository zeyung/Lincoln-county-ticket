{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "318498f4-7f14-42a7-bbfd-6ef83d3349b7",
      "metadata": {
        "id": "318498f4-7f14-42a7-bbfd-6ef83d3349b7"
      },
      "source": [
        "# INFO 159/259"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9037f2a-d06a-4c4c-ba71-d571e5873c01",
      "metadata": {
        "id": "e9037f2a-d06a-4c4c-ba71-d571e5873c01"
      },
      "source": [
        "# <center> Homework 1: Word Embeddings </center>\n",
        "<center> Due: February 3, 2026 @ 11:59pm </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47db1e18-852a-46ee-8ae9-c9b1b8682320",
      "metadata": {
        "id": "47db1e18-852a-46ee-8ae9-c9b1b8682320"
      },
      "source": [
        "# HW1: Word Embeddings\n",
        "\n",
        "In this homework, you will implement _word2vec_ with skip-grams and negative sampling, training on a small slice of Wikipedia data.\n",
        "\n",
        "*Learning objectives*:\n",
        "- Understand the implementation details of _word2vec_\n",
        "- Gain familiarity with `numpy` for matrix math\n",
        "- Gain familiarity with training a classifier using stochastic gradient descent.\n",
        "\n",
        "You may want to consult SLP chapter 5 (_Embeddings_) as a reference for the implementation. This homework is designed to run on the CPU only, so if you are using Google Colab, you may want to ensure that your CPU is selected (under `Runtime > Change runtime type` in the top bar) so that you save your GPU allocation for later assignments in the semester."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6017ce58-b658-47be-a26b-2b08cfa5a696",
      "metadata": {
        "id": "6017ce58-b658-47be-a26b-2b08cfa5a696"
      },
      "outputs": [],
      "source": [
        "# download the dataset we will be using\n",
        "!wget https://github.com/dbamman/nlp-course/raw/refs/heads/main/HW/data/en_wiki_sample.txt -O en_wiki_sample.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32448ff3-a870-4621-9f9d-a1e83f3bc641",
      "metadata": {
        "id": "32448ff3-a870-4621-9f9d-a1e83f3bc641"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tqdm import tqdm\n",
        "\n",
        "nltk.download(\"punkt_tab\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b6563cd-75ec-472f-bc73-d0af6fd4e68e",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "9b6563cd-75ec-472f-bc73-d0af6fd4e68e"
      },
      "source": [
        "## Data loading\n",
        "\n",
        "We will begin by loading and tokenizing the data. The file contains a list of paragraphs from Wikipeda, separated by newlines. Because each document (a paragraph) is sampled independently, we want to maintain the document boundaries when we sample contexts later.\n",
        "\n",
        "Inside `FileDataLoader`:\n",
        "- `idx2vocab` is a list of unique word types\n",
        "- `vocab2idx` is a dict mapping from a word type to its index in `idx2vocab`\n",
        "- `word_freqs` is a dict mapping from a word type to its frequency in the corpus\n",
        "\n",
        "You should implement:\n",
        "1. The `negative_sample_weights()` function\n",
        "\n",
        "   This function should calculate the weighted sample probabilities for each of the words in our vocabulary.\n",
        "   Recall SLP3 eq. 5.19:\n",
        "    $$\n",
        "     P_{\\alpha}(w) = \\frac{\\text{count}(w)^{\\alpha}}{\\sum_{w'}\\text{count}(w')^{\\alpha}}\n",
        "    $$\n",
        "   We calculate and store the sample weights to save time when generating contexts later.\n",
        "3. The `negative_sample()` function\n",
        "\n",
        "   This function should sample `num_samples` negative context words given a target word. Recall from SLP3 5.5.2\n",
        "   > A noise word is a random word from the lexicon, **constrained not to be the target word $w$**. (_emph added_)\n",
        "\n",
        "   So, when sampling, you will want to copy the original `.sample_weights` numpy array, set the probability of the target word to 0, and renormalize the weights before sampling.\n",
        "\n",
        "   You may want to consult the numpy documentation for [`numpy.random.Generator.choice()`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html#numpy.random.Generator.choice). We have instantiated a random generator for your convenience in `self.rng`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe878ae3-b7ff-45a3-9a85-1dcb2a9b001e",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "fe878ae3-b7ff-45a3-9a85-1dcb2a9b001e"
      },
      "source": [
        "_Learning objectives_:\n",
        "> - Understand the implementation details of _word2vec_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7411592b-c7af-4fc2-a332-ccd22c87c26e",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "7411592b-c7af-4fc2-a332-ccd22c87c26e"
      },
      "outputs": [],
      "source": [
        "corpus_path = \"./en_wiki_sample.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "df7830a5-b148-4002-b927-73f7bcf3d005",
      "metadata": {
        "tags": [
          "otter_answer_cell"
        ],
        "id": "df7830a5-b148-4002-b927-73f7bcf3d005"
      },
      "outputs": [],
      "source": [
        "class FileDataLoader():\n",
        "    def __init__(self, filepath, negative_sample_alpha=0.75, min_threshold=5):\n",
        "        self.negative_sample_alpha = negative_sample_alpha\n",
        "        self.min_threshold = min_threshold\n",
        "\n",
        "        self.tokenized_documents = self.load_data(filepath)\n",
        "        self.word_freqs = self.get_word_freqs(self.tokenized_documents)\n",
        "\n",
        "        # replace words that appear fewer than min_threshold times with an [UNK] token\n",
        "        for word, freq in list(self.word_freqs.items()):\n",
        "            if freq < min_threshold:\n",
        "                self.word_freqs[\"[UNK]\"] += freq\n",
        "                del self.word_freqs[word]\n",
        "\n",
        "        self.idx2vocab = list(self.word_freqs.keys())\n",
        "        self.vocab2idx = {word: index for index, word in enumerate(self.idx2vocab)}\n",
        "\n",
        "        # set up a random number generator we can use for sampling\n",
        "        self.rng = np.random.default_rng(159259)\n",
        "        self.sample_weights = self.negative_sample_weights(alpha=negative_sample_alpha)\n",
        "\n",
        "        ...\n",
        "\n",
        "    def tokenize_and_lowercase(self, doc):\n",
        "        \"\"\"Tokenize a doc and lowercase all the words.\"\"\"\n",
        "        return [word.lower() for word in word_tokenize(doc)]\n",
        "\n",
        "    def get_word_freqs(self, tokenized_documents):\n",
        "        \"\"\"Return a dictionary mapping each word to its frequency.\"\"\"\n",
        "        return Counter(itertools.chain.from_iterable(tokenized_documents))\n",
        "\n",
        "    def load_data(self, filepath):\n",
        "        return [self.tokenize_and_lowercase(doc) for doc in tqdm(open(corpus_path, \"r\").readlines())]\n",
        "\n",
        "    def negative_sample_weights(self, alpha):\n",
        "        \"\"\"Calculate the weighted probabilities of each word.\n",
        "\n",
        "        Return a (v,)-shaped numpy array, where v is the size of the vocabulary.\n",
        "        \"\"\"\n",
        "\n",
        "        counts = np.array([self.word_freqs[w] for w in self.idx2vocab])\n",
        "\n",
        "\n",
        "        counts_pow = np.power(counts, alpha)\n",
        "\n",
        "\n",
        "        return counts_pow / np.sum(counts_pow)\n",
        "\n",
        "    def negative_sample(self, target_word_idx, num_samples):\n",
        "        \"\"\"Sample num_samples noise words from the lexicon that is not the target word.\n",
        "\n",
        "        The sample probabilities should be proportional to their weighted unigram probability if the target word probability is set to 0.\n",
        "\n",
        "        Return a (num_samples,)-shaped numpy array of sampled indices.\n",
        "        \"\"\"\n",
        "        # TODO: implement this function\n",
        "        samples = np.random.Generator.choice(self.idx2vocab, num_samples, p=self.sample_weights)\n",
        "        return samples\n",
        "\n",
        "    def sample_contexts(self, window_size, sample_k):\n",
        "        for doc in self.tokenized_documents:\n",
        "            if len(doc) < (2 * window_size) + 1:\n",
        "                # the doc is too short for our desired window size; we skip it\n",
        "                continue\n",
        "            for word_idx in range(window_size, len(doc) - window_size):\n",
        "                target_word_idx = self.vocab2idx[doc[word_idx]] if doc[word_idx] in self.vocab2idx else self.vocab2idx[\"[UNK]\"]\n",
        "                # sample positive words from the window\n",
        "                positive_word_idxs = np.array([\n",
        "                    self.vocab2idx[word] if word in self.vocab2idx else self.vocab2idx[\"[UNK]\"] for word in doc[word_idx - window_size:word_idx] + doc[word_idx + 1:word_idx + 1 + window_size]\n",
        "\n",
        "                ])\n",
        "                # sample len(positive_word_idxs) * sample_k number of negative words\n",
        "                negative_word_idxs = self.negative_sample(target_word_idx, sample_k * len(positive_word_idxs))\n",
        "                yield (target_word_idx, positive_word_idxs, negative_word_idxs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ee0d413e-3180-4601-9e6a-3ce2c9cbe07b",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee0d413e-3180-4601-9e6a-3ce2c9cbe07b",
        "outputId": "0bc1e939-4e97-4753-85de-088974a2287a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100000/100000 [00:52<00:00, 1891.49it/s]\n"
          ]
        }
      ],
      "source": [
        "# this should take roughly 30 seconds\n",
        "dataloader = FileDataLoader(corpus_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c4c9e9e-9515-4283-92e6-2aad6065e023",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "8c4c9e9e-9515-4283-92e6-2aad6065e023"
      },
      "source": [
        "**Quick check**: The unweighted probability for \"the\" should be 0.063; the weighted probability should be 0.016."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "82045973-aa69-4fce-b055-db8d9bbdd47e",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82045973-aa69-4fce-b055-db8d9bbdd47e",
        "outputId": "af9fbc74-0681-48ff-bc71-0e69ec74647a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unweighted probability for `the`: \t\t0.063\n",
            "Weighted (alpha=0.75) probability for `the`: \t0.016\n"
          ]
        }
      ],
      "source": [
        "print(f\"Unweighted probability for `the`: \\t\\t{dataloader.word_freqs['the'] / dataloader.word_freqs.total():.3f}\")\n",
        "print(f\"Weighted (alpha=0.75) probability for `the`: \\t{dataloader.sample_weights[dataloader.vocab2idx['the']]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77ac31be-c0cf-4fe3-a6e1-5494f4c596d2",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "77ac31be-c0cf-4fe3-a6e1-5494f4c596d2"
      },
      "source": [
        "## Setting up the model\n",
        "\n",
        "The word2vec model consists of two matrices: the target (or input) embedding and the context (or output) embedding. We set those up here.\n",
        "\n",
        "You should implement:\n",
        "- The `nearest_neighbors()` function\n",
        "\n",
        "  This given a $d$-dimensional $\\vec{v}$ and a $(v \\times d)$-dimensional matrix $M$ of vectors to query against, we want to calculate the cosine similarity of $\\vec{v}$ with each row of $M$ and return the indices (and the corresponding similarities) of the most similar rows in $M$.\n",
        "\n",
        "  As a reminder, the cosine similarity of two vectors $\\vec{a}$ and $\\vec{b}$ is\n",
        "  $$\n",
        "    \\text{cosine\\_sim}(\\vec{a}, \\vec{b}) = \\frac{\\vec{a} \\cdot \\vec{b}}{\\|{\\vec{a}}\\|\\|\\vec{b}\\|}\n",
        "  $$\n",
        "\n",
        "  This is derived from one of the formulations for the dot product:\n",
        "  $$\n",
        "    \\vec{a} \\cdot \\vec{b} = \\|\\vec{a}\\| \\|\\vec{b}\\| \\cos({\\theta})\n",
        "  $$\n",
        "\n",
        "  $\\|\\vec{a}\\|$ denotes the $l_2$-norm of a vector, or its magnitude.\n",
        "\n",
        "  You might want to consult the numpy documentation for [`numpy.matmul`](https://numpy.org/doc/2.1/reference/generated/numpy.matmul.html), [`numpy.argsort`](https://numpy.org/doc/2.1/reference/generated/numpy.argsort.html#numpy-argsort), and [`numpy.linalg.norm`](https://numpy.org/doc/2.1/reference/generated/numpy.linalg.norm.html)\n",
        "\n",
        "\n",
        "_Learning objectives_:\n",
        "> - Gain familiarity with `numpy` for matrix math\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "786c582e-c88c-45d0-944f-3cf2b6e165bf",
      "metadata": {
        "tags": [
          "otter_answer_cell"
        ],
        "id": "786c582e-c88c-45d0-944f-3cf2b6e165bf"
      },
      "outputs": [],
      "source": [
        "class Word2Vec():\n",
        "    def __init__(self, dataloader, hidden_dim=100):\n",
        "        self.dataloader = dataloader\n",
        "        self.vocab_size = len(self.dataloader.idx2vocab)\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        np.random.seed(159259)\n",
        "        # We initialize the model weights to be uniformly randomly distributed and centered around 0.\n",
        "        self.target_embs = (np.random.random((self.vocab_size, hidden_dim)) - 0.5) / hidden_dim\n",
        "        self.context_embs = (np.random.random((self.vocab_size, hidden_dim)) - 0.5) / hidden_dim\n",
        "\n",
        "    def nearest_neighbors(self, query_vector, vectors, n=10):\n",
        "        \"\"\"Finds the `n` indices of the rows in `vectors` that have the highest cosine similarity to `query_vector`.\n",
        "\n",
        "        query_vector: (d,)-shaped numpy array\n",
        "        vectors: (v, d)-shaped numpy array\n",
        "        n: int\n",
        "\n",
        "        Return a tuple of (indices, similarities), where both are (n,)-shaped ndarrays.\n",
        "        \"\"\"\n",
        "        ...\n",
        "\n",
        "    def print_nearest_neighbors(self, word, n=5):\n",
        "        \"\"\"Prints the `n` nearest neighbors for a word using the context embeddings.\n",
        "\n",
        "        word: str\n",
        "\n",
        "        Return None\n",
        "        \"\"\"\n",
        "        query_vector = self.context_embs[self.dataloader.vocab2idx[word]]\n",
        "        closest_inds, similarities = self.nearest_neighbors(query_vector, self.context_embs, n)\n",
        "        words = [self.dataloader.idx2vocab[ind] for ind in closest_inds]\n",
        "\n",
        "        print(words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c86a648f-9671-4f85-8644-a095f9dfe708",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "c86a648f-9671-4f85-8644-a095f9dfe708"
      },
      "outputs": [],
      "source": [
        "w2v_model = Word2Vec(dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf8bc322-4dad-4810-b4c1-eea1298c4042",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "cf8bc322-4dad-4810-b4c1-eea1298c4042"
      },
      "source": [
        "**Quick check**: you can check your function against this toy example. The output should be:\n",
        "\n",
        "- `(array([4, 5, 0, 6, 3]), array([0.91347529, 0.87409283, 0.84518755, 0.83396453, 0.8111933 ]))`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3a58b3e-b51b-423e-bf1e-71764ef0d9fb",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "c3a58b3e-b51b-423e-bf1e-71764ef0d9fb"
      },
      "outputs": [],
      "source": [
        "def quick_check():\n",
        "    np.random.seed(159259)\n",
        "    query_vec = np.random.random(size=(5,))\n",
        "    other_vecs = np.random.random(size=(10, 5))\n",
        "    print(w2v_model.nearest_neighbors(query_vec, other_vecs, n=5))\n",
        "\n",
        "quick_check()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd6196d4-bf38-4de1-b3fd-6ca986056a7a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "cd6196d4-bf38-4de1-b3fd-6ca986056a7a"
      },
      "source": [
        "**Quick check**: the nearest neighbors for \"the\" should be random at this point; if you did not edit the `__init__` function, the nearest neighbors should be:\n",
        "\n",
        "- `['the', 'asian', 'habilitation', 'toward', 'capacity-building']`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcd4e8e8-10a2-4a02-8fcd-cf05d63624b0",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "bcd4e8e8-10a2-4a02-8fcd-cf05d63624b0"
      },
      "outputs": [],
      "source": [
        "w2v_model.print_nearest_neighbors(\"the\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79dab544-e659-434e-8ef0-74b620cccc9d",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "79dab544-e659-434e-8ef0-74b620cccc9d"
      },
      "source": [
        "## Setting up the training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77f9f4b2-9e29-4d62-8d5e-0e04797fd2b1",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "77f9f4b2-9e29-4d62-8d5e-0e04797fd2b1"
      },
      "source": [
        "### Calculating gradients\n",
        "\n",
        "To update the weights using gradient descent, we have to find the partial derivatives of the loss with respect to the parameters. You can find the loss function and its partial derivatives in SLP 5.5.2 (eqs. 5.22 - 5.24); we've also reproduced them for you below. While we give you the derivatives, it can be a good exercise to try to derive them yourself!\n",
        "\n",
        "These rely on the sigmoid function, which we've implemented for you as an example.\n",
        "\n",
        "You should implement:\n",
        "- `loss_fn`\n",
        "- `c_pos_grad`\n",
        "- `c_neg_grad`\n",
        "- `w_grad`\n",
        "\n",
        "In each of these functions, you should expect:\n",
        "- `w` to be a `d`-dimensional vector,\n",
        "- `c_pos` to be a `(n_pos, d)`-dimensional matrix (where `n_pos` is the number of positive context examples)\n",
        "- `c_neg` to be a `(n_neg, d)`-dimensional matrix (where `n_neg` is the number of negative context examples)\n",
        "\n",
        "As a reminder, the sigmoid function is defined as\n",
        "$$\n",
        "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
        "$$\n",
        "\n",
        "For filling out the rest of the functions, you may want to use [`np.log`](https://numpy.org/devdocs/reference/generated/numpy.log.html#numpy.log), [`np.sum`](https://numpy.org/devdocs/reference/generated/numpy.sum.html), [`np.newaxis`](https://numpy.org/devdocs/reference/constants.html#numpy.newaxis), [`np.matmul`](https://numpy.org/devdocs/reference/generated/numpy.matmul.html#numpy-matmul), and of course, the `sigmoid` function that we have implemented for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca4fe294-5650-4bca-a43a-787ac3df9125",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "ca4fe294-5650-4bca-a43a-787ac3df9125"
      },
      "outputs": [],
      "source": [
        "# we wrap these functions in the @njit decorator to speed up calculations\n",
        "# using just-in-time compilation\n",
        "# you don't have to worry about this\n",
        "from numba import njit\n",
        "\n",
        "@njit\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f07f23e3-aea8-4e7e-8fd9-7e163082463a",
      "metadata": {
        "tags": [
          "otter_answer_cell"
        ],
        "id": "f07f23e3-aea8-4e7e-8fd9-7e163082463a"
      },
      "outputs": [],
      "source": [
        "@njit\n",
        "def loss_fn(w, c_pos, c_neg):\n",
        "    ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62d4d438-55d4-4679-bcd6-085eb8a4e0b7",
      "metadata": {
        "tags": [
          "otter_answer_cell"
        ],
        "id": "62d4d438-55d4-4679-bcd6-085eb8a4e0b7"
      },
      "outputs": [],
      "source": [
        "@njit\n",
        "def c_pos_grad(w, c_pos):\n",
        "    ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b76a8d2c-f760-48c4-bef3-b39de087d9b2",
      "metadata": {
        "tags": [
          "otter_answer_cell"
        ],
        "id": "b76a8d2c-f760-48c4-bef3-b39de087d9b2"
      },
      "outputs": [],
      "source": [
        "@njit\n",
        "def c_neg_grad(w, c_neg):\n",
        "    ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e99ed4d6-fff8-4984-9a02-4da80a63eb97",
      "metadata": {
        "tags": [
          "otter_answer_cell"
        ],
        "id": "e99ed4d6-fff8-4984-9a02-4da80a63eb97"
      },
      "outputs": [],
      "source": [
        "@njit\n",
        "def w_grad(w, c_pos, c_neg):\n",
        "    ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd707bf6-2271-41ae-a2d8-219b69fe5c3f",
      "metadata": {
        "deletable": false,
        "editable": false,
        "jp-MarkdownHeadingCollapsed": true,
        "id": "fd707bf6-2271-41ae-a2d8-219b69fe5c3f"
      },
      "source": [
        "**(Not so) Quick check**: We can check the correctness of the loss function and gradient calculations by numerically approximating the gradients using neighboring points and seeing if they match up. Recall from your calculus class:\n",
        "\n",
        "$$\n",
        "\\frac{d}{dx} f(x) = \\lim_{h \\to 0} \\frac{f(x + h) - f(x - h)}{2h}\n",
        "$$\n",
        "\n",
        "We implement this in the `approximate_gradient` function so that we can estimate the local gradient and see if the closed-form solution that you implemented in the functions above are accurate. However, we never numerically approximate the gradient during training because we have a closed-form solution that is both more accurate and more efficient to calculate.\n",
        "\n",
        "> **Aside**: In this assignment, we have you manually calculate the loss and gradients. If you have taken other deep learning classes, you may have experience with libraries like Pytorch, which implement automatic differentiation so that you can just specify the loss function and not have to work out the gradients manually.\n",
        ">\n",
        "> These libraries _don't_ use numerical approximation for the gradients. Instead, they rely on the chain rule:\n",
        ">\n",
        "> $$\n",
        "    \\frac{d}{dx} f(g(x)) = f'(g(x)) g'(x)\n",
        "  $$\n",
        "> As long as all of the functions you apply to an input are differentiable, and the closed-form derivatives are known (which they often are, since most functions break down into basic differentiable operations like addition, multiplication, or exponentiation), the library can construct a graph to track all of the applications of the functions and calculate the partial derivatives using this graph.\\\n",
        ">\n",
        "> You can read more about this in the [Pytorch autograd tutorial](https://docs.pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#computational-graph).\n",
        "\n",
        "Your loss should be roughly 8.05; if it is not, all of the assertions in the `quick_check` will likely fail even if (especially if) your gradients are implemented correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fecb738-819b-42d3-a417-8ef633c3864f",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "4fecb738-819b-42d3-a417-8ef633c3864f"
      },
      "outputs": [],
      "source": [
        "def quick_check():\n",
        "    np.random.seed(159259)\n",
        "\n",
        "    w = np.random.random((5,))\n",
        "    c_pos = np.random.random((2, 5))\n",
        "    c_neg = np.random.random((4, 5))\n",
        "\n",
        "    eps = 1e-5\n",
        "\n",
        "    def approximate_gradient(func, vec, eps=1e-5):\n",
        "        est_grad = np.zeros(vec.shape)\n",
        "        for ind, el in np.ndenumerate(vec):\n",
        "            perturb = np.zeros(vec.shape)\n",
        "            perturb[ind] = eps\n",
        "            est_grad[ind] = (func(vec + perturb) - func(vec - perturb)) / (2 * eps)\n",
        "        return est_grad\n",
        "\n",
        "    print(\"loss:\", loss_fn(w, c_pos, c_neg))\n",
        "\n",
        "    assert np.allclose(w_grad(w, c_pos, c_neg), approximate_gradient(lambda x: loss_fn(x, c_pos, c_neg), w)), \"c_pos_grad is not correct for loss_fn\"\n",
        "    assert np.allclose(c_pos_grad(w, c_pos), approximate_gradient(lambda x: loss_fn(w, x, c_neg), c_pos)), \"c_pos_grad is not correct for loss_fn\"\n",
        "    assert np.allclose(c_neg_grad(w, c_neg), approximate_gradient(lambda x: loss_fn(w, c_pos, x), c_neg)), \"c_neg_grad is not correct for loss_fn\"\n",
        "\n",
        "quick_check()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fd3d896-dace-4d50-b0c3-f065af19305b",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "7fd3d896-dace-4d50-b0c3-f065af19305b"
      },
      "source": [
        "### Updating weights in the training loop\n",
        "\n",
        "The training loop for SGD consists of sampling one instance of the data (in our case, a target word and its positive and negative contexts), and calculating the partial derivatives of the loss.\n",
        "\n",
        "We then update the parameters using these partial derivatives, multiplying each gradient by the learning rate $\\eta$. When we perform gradient descent, we subtract the gradients from the weights in order to shift the weights in a direction that decreases the loss (locally, at least). Here are the updates we make:\n",
        "$$\n",
        "c_{\\text{pos}}^{t + 1} = c_{\\text{pos}}^{t} - \\eta \\frac{\\partial L}{\\partial {c}_{\\text{pos}}^t},\n",
        "$$\n",
        "$$\n",
        "c_{\\text{neg}}^{t + 1} = c_{\\text{neg}}^{t} - \\eta \\frac{\\partial L}{\\partial {c}_{\\text{neg}}^t}\n",
        ",$$\n",
        "$$\n",
        "w^{t + 1} = w^{t} - \\eta \\frac{\\partial L}{\\partial w^t}\n",
        ",$$\n",
        "where $t + 1$ is the next timestep in the stochastic gradient descent loop.\n",
        "\n",
        "**Note**: We print some diagnostic information, including the loss, to help you monitor the training. You should convince yourself that, though we calculate the loss and print it here to track our training, SGD doesn't actually require that we compute the loss as such; we really only need the gradients.\n",
        "\n",
        "You implement:\n",
        "- the section of the code where you calculate the gradients\n",
        "- the section of the code where you use the gradients to update the embedding\n",
        "\n",
        "You may want to read about [numpy indexing](https://numpy.org/doc/2.2/user/basics.indexing.html#), since the `.sample_contexts()` returns lists of indices; you might also want to look into [`np.subtract.at()`](https://numpy.org/doc/2.2/reference/generated/numpy.ufunc.at.html) (see the usage of `np.add.at()` in the starter code as another example).\n",
        "\n",
        "With a learning rate of 0.01, you should see some nearest neighbors start to make sense after about the loss drops under 60 or so. This took around 60K steps and 1m21s on our solution code; we recommend running for at least 10 minutes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a9ac359-0e50-4a96-8544-4bcf4ce2700a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "8a9ac359-0e50-4a96-8544-4bcf4ce2700a"
      },
      "source": [
        "_Learning objectives_:\n",
        "> - Gain familiarity with training a classifier using stochastic gradient descent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6a656a1-706f-46e2-8ecf-0f684fd04e3c",
      "metadata": {
        "tags": [
          "otter_answer_cell"
        ],
        "id": "f6a656a1-706f-46e2-8ecf-0f684fd04e3c"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 1\n",
        "LEARNING_RATE = 0.01\n",
        "\n",
        "def train(model, dataloader):\n",
        "\n",
        "    num_target_updates = np.zeros((model.target_embs.shape[0],))\n",
        "    num_context_updates = np.zeros((model.context_embs.shape[0],))\n",
        "\n",
        "    def print_diagnostic(word):\n",
        "        print(f\"`{word}` was updated {int(num_target_updates[dataloader.vocab2idx[word]])} times in target and {int(num_context_updates[dataloader.vocab2idx[word]])} times in context\")\n",
        "        model.print_nearest_neighbors(word, 4)\n",
        "\n",
        "    for i in range(NUM_EPOCHS):\n",
        "        losses = []\n",
        "        for i, (target, pos, neg) in enumerate(tqdm(dataloader.sample_contexts(window_size=2, sample_k=100))):\n",
        "\n",
        "            if i % 10_000 == 0:\n",
        "                # Print diagnostic info every 10_000 steps.\n",
        "                print(\"avg loss:\", sum(losses) / len(losses) if losses else \"\")\n",
        "                losses = []\n",
        "                print_diagnostic(\"he\")\n",
        "                print_diagnostic(\"original\")\n",
        "                print_diagnostic(\"january\")\n",
        "\n",
        "            # Get the vectors from the model\n",
        "            w = model.target_embs[target]\n",
        "            c_pos = model.context_embs[pos]\n",
        "            c_neg = model.context_embs[neg]\n",
        "\n",
        "            # Calculate and store the loss\n",
        "            losses.append(loss_fn(w, c_pos, c_neg))\n",
        "\n",
        "            # TODO: Calculate the gradients and implement the gradient update.\n",
        "            ...\n",
        "\n",
        "            # Tally up how many times each word has been seen, just for fun.\n",
        "            np.add.at(num_target_updates, target, 1)\n",
        "            np.add.at(num_context_updates, pos, 1)\n",
        "            np.add.at(num_context_updates, neg, 1)\n",
        "\n",
        "w2v_model = Word2Vec(dataloader)\n",
        "train(w2v_model, dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d25f981-30ac-4df4-99af-70c78619dcd7",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "8d25f981-30ac-4df4-99af-70c78619dcd7"
      },
      "source": [
        "Once you are satisfied with the training (you can stop it whenever you want), experiment with printing out some nearest neighbors. Do these align with your expectations? Do any surprise you?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b5caadc-5b40-4119-aa7b-075f12ab2055",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "7b5caadc-5b40-4119-aa7b-075f12ab2055"
      },
      "outputs": [],
      "source": [
        "model.print_nearest_neighbors(\"paris\", 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94ef471a-ff16-4d20-923f-7a5aea813a16",
      "metadata": {
        "id": "94ef471a-ff16-4d20-923f-7a5aea813a16"
      },
      "source": [
        "## Submission"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c000e0b3-1ef4-4ba9-b3b5-cc4690a92ca5",
      "metadata": {
        "id": "c000e0b3-1ef4-4ba9-b3b5-cc4690a92ca5"
      },
      "source": [
        "Congratulations on finishing HW1!\n",
        "\n",
        "Please ensure that you submit a PDF of this notebook onto [Gradescope](https://www.gradescope.com/courses/1238346) before February 3 at 11:59pm."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9589c7fa-68cf-4bee-88b7-78bf92c2dc89",
      "metadata": {
        "id": "9589c7fa-68cf-4bee-88b7-78bf92c2dc89"
      },
      "source": [
        "You can run the cell below to generate a PDF if you are using Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b252b823-44c7-4d4a-ae50-03cabaac3913",
      "metadata": {
        "id": "b252b823-44c7-4d4a-ae50-03cabaac3913"
      },
      "outputs": [],
      "source": [
        "#EXPORT_EXCLUDE#\n",
        "\n",
        "#@markdown This is a helper function to generate a PDF in Colab.\n",
        "#@markdown If you are using Jupyter notebook, you can do `File > Save and Export Notebook as HTML`, then save the resulting HTML file as a PDF.\n",
        "#@markdown Alternatively, in Juypter notebook, you might try `File > Save and Export Notebook as PDF`, but just make sure you already have `pandoc` installed.\n",
        "\n",
        "def colab_export_pdf():\n",
        "    # Modified from: https://medium.com/@jonathanagustin/convert-colab-notebook-to-pdf-0ccd8f847dd6\n",
        "    try:\n",
        "        import google.colab\n",
        "        IN_COLAB = True\n",
        "    except:\n",
        "        IN_COLAB = False\n",
        "        print(\"This cell only works in Google Colab!\")\n",
        "        print(\"If you are running locally, click File > Export as HTML. Then open the HTML file and save it as a PDF.\")\n",
        "\n",
        "    if IN_COLAB:\n",
        "        print(\"Generating PDF. This may take a few seconds.\")\n",
        "        import os, datetime, json, locale, pathlib, urllib, requests, werkzeug, nbformat, google, yaml, warnings\n",
        "        locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\n",
        "        NAME = pathlib.Path(werkzeug.utils.secure_filename(urllib.parse.unquote(requests.get(f\"http://{os.environ['COLAB_JUPYTER_IP']}:{os.environ['KMP_TARGET_PORT']}/api/sessions\").json()[0][\"name\"])))\n",
        "        TEMP = pathlib.Path(\"/content/pdfs\") / f\"{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}_{NAME.stem}\"; TEMP.mkdir(parents=True, exist_ok=True)\n",
        "        NB = [cell for cell in nbformat.reads(json.dumps(google.colab._message.blocking_request(\"get_ipynb\", timeout_sec=30)[\"ipynb\"]), as_version=4).cells if \"--Colab2PDF\" not in cell.source]\n",
        "        warnings.filterwarnings('ignore', category=nbformat.validator.MissingIDFieldWarning)\n",
        "        with (TEMP / f\"{NAME.stem}.ipynb\").open(\"w\", encoding=\"utf-8\") as nb_copy: nbformat.write(nbformat.v4.new_notebook(cells=NB or [nbformat.v4.new_code_cell(\"#\")]), nb_copy)\n",
        "        if not pathlib.Path(\"/usr/local/bin/quarto\").exists():\n",
        "            !wget -q \"https://quarto.org/download/latest/quarto-linux-amd64.deb\" -P {TEMP} && dpkg -i {TEMP}/quarto-linux-amd64.deb > /dev/null && quarto install tinytex --update-path --quiet\n",
        "        with (TEMP / \"config.yml\").open(\"w\", encoding=\"utf-8\") as file: yaml.dump({'include-in-header': [{\"text\": r\"\\usepackage{fvextra}\\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaksymbolleft={},showspaces=false,showtabs=false,breaklines,breakanywhere,commandchars=\\\\\\{\\}}\"}],'include-before-body': [{\"text\": r\"\\DefineVerbatimEnvironment{verbatim}{Verbatim}{breaksymbolleft={},showspaces=false,showtabs=false,breaklines}\"}]}, file)\n",
        "        !quarto render {TEMP}/{NAME.stem}.ipynb --metadata-file={TEMP}/config.yml --to pdf -M latex-auto-install -M margin-top=1in -M margin-bottom=1in -M margin-left=1in -M margin-right=1in --quiet\n",
        "        google.colab.files.download(str(TEMP / f\"{NAME.stem}.pdf\"))\n",
        "\n",
        "colab_export_pdf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5682dd69-85ff-4f21-84e6-64ee3146cdb7",
      "metadata": {
        "id": "5682dd69-85ff-4f21-84e6-64ee3146cdb7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "otter": {
      "OK_FORMAT": true,
      "tests": {}
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}